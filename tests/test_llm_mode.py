"""
Change Analyzer + Document Generator LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸

ì‹¤ì œ OpenAI APIë¥¼ ì‚¬ìš©í•˜ì—¬ LLM ê¸°ë°˜ ë¶„ì„ ë° ë¬¸ì„œ ìƒì„±ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆë“¤
import sys
sys.path.append('.')
from domain.langgraph.nodes.change_analyzer_node import change_analyzer_node
from domain.langgraph.nodes.document_generator_node import document_generator_node
from domain.langgraph.document_state import DocumentState
from langchain_openai import ChatOpenAI

class TestLLMMode:
    """LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_results_dir = Path("test_results")
        self.test_results_dir.mkdir(exist_ok=True)
        self.test_session = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # .env íŒŒì¼ ë¡œë“œ
        from dotenv import load_dotenv
        load_dotenv()
        
        # API í‚¤ í™•ì¸
        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            api_key=lambda: self.api_key,
            model="gpt-4o-mini",  # ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ mini ì‚¬ìš©
            temperature=0.1
        )
        print(f"âœ… LLM initialized with model: gpt-4o-mini")
        
    def save_test_result(self, test_name: str, result: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        result_file = self.test_results_dir / f"{self.test_session}_llm_{test_name}.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        print(f"âœ… Test result saved: {result_file}")

    def create_realistic_state(self, scenario: str) -> DocumentState:
        """ì‹¤ì œì ì¸ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±"""
        base_state: DocumentState = {
            "code_change_id": 1,
            "status": "analyzing",
            "should_update": False,
        }
        
        if scenario == "feature_auth":
            base_state.update({
                "changed_files": [
                    "src/auth/authentication.py", 
                    "src/auth/jwt_handler.py",
                    "src/models/user.py",
                    "tests/test_auth.py"
                ],
                "code_change": {
                    "commit_sha": "a1b2c3d4",
                    "commit_message": "feat: implement JWT-based user authentication system",
                    "diff_content": """
+class JWTHandler:
+    def __init__(self, secret_key: str, algorithm: str = "HS256"):
+        self.secret_key = secret_key
+        self.algorithm = algorithm
+    
+    def encode_token(self, payload: dict) -> str:
+        \"\"\"JWT í† í°ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"
+        payload['exp'] = datetime.utcnow() + timedelta(hours=24)
+        return jwt.encode(payload, self.secret_key, algorithm=self.algorithm)
+    
+    def decode_token(self, token: str) -> dict:
+        \"\"\"JWT í† í°ì„ ê²€ì¦í•˜ê³  ë””ì½”ë”©í•©ë‹ˆë‹¤.\"\"\"
+        try:
+            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
+            return payload
+        except jwt.ExpiredSignatureError:
+            raise HTTPException(status_code=401, detail="Token has expired")
+        except jwt.InvalidTokenError:
+            raise HTTPException(status_code=401, detail="Invalid token")

+def authenticate_user(credentials: UserCredentials) -> AuthResponse:
+    \"\"\"ì‚¬ìš©ì ì¸ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"
+    user = get_user_by_email(credentials.email)
+    if not user or not verify_password(credentials.password, user.hashed_password):
+        raise HTTPException(status_code=401, detail="Invalid credentials")
+    
+    access_token = jwt_handler.encode_token({"sub": user.id, "email": user.email})
+    return AuthResponse(access_token=access_token, token_type="bearer")
                    """
                }
            })
        elif scenario == "bugfix_database":
            base_state.update({
                "changed_files": [
                    "src/database/connection.py",
                    "src/utils/retry.py"
                ],
                "code_change": {
                    "commit_sha": "e5f6g7h8",
                    "commit_message": "fix: resolve database connection pool exhaustion issue",
                    "diff_content": """
-DATABASE_URL = "postgresql://user:pass@localhost/db"
-engine = create_engine(DATABASE_URL)
+DATABASE_URL = "postgresql://user:pass@localhost/db"
+engine = create_engine(
+    DATABASE_URL,
+    pool_size=20,
+    max_overflow=30,
+    pool_pre_ping=True,
+    pool_recycle=3600
+)

+@retry(max_attempts=3, delay=1.0)
+def get_database_connection():
+    \"\"\"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤.\"\"\"
+    try:
+        connection = engine.connect()
+        # ì—°ê²° í…ŒìŠ¤íŠ¸
+        connection.execute(text("SELECT 1"))
+        return connection
+    except SQLAlchemyError as e:
+        logger.error(f"Database connection failed: {e}")
+        raise
                    """
                }
            })
        elif scenario == "refactor_api":
            base_state.update({
                "changed_files": [
                    "src/api/v1/users.py",
                    "src/api/v1/orders.py", 
                    "src/schemas/response.py",
                    "src/middleware/validation.py"
                ],
                "code_change": {
                    "commit_sha": "i9j0k1l2",
                    "commit_message": "refactor: standardize API response format and add request validation",
                    "diff_content": """
+class APIResponse(BaseModel):
+    \"\"\"í‘œì¤€ API ì‘ë‹µ í˜•ì‹\"\"\"
+    success: bool
+    data: Optional[Any] = None
+    message: str = ""
+    errors: Optional[List[str]] = None
+    timestamp: datetime = Field(default_factory=datetime.utcnow)

-@app.get("/users/{user_id}")
-def get_user(user_id: int):
-    user = db.query(User).filter(User.id == user_id).first()
-    if not user:
-        return {"error": "User not found"}
-    return {"user": user}
+@app.get("/users/{user_id}", response_model=APIResponse)
+def get_user(user_id: int = Path(..., gt=0)):
+    try:
+        user = db.query(User).filter(User.id == user_id).first()
+        if not user:
+            return APIResponse(success=False, message="User not found")
+        return APIResponse(success=True, data=user, message="User retrieved successfully")
+    except Exception as e:
+        return APIResponse(success=False, message="Internal server error", errors=[str(e)])
                    """
                }
            })
        elif scenario == "document_update":
            base_state.update({
                "should_update": True,
                "changed_files": ["README.md", "docs/deployment.md"],
                "existing_document": {
                    "id": 1,
                    "title": "FastAPI Authentication Service Documentation",
                    "content": """# FastAPI Authentication Service Documentation

## Overview
This service provides JWT-based authentication for web applications.

## Features
- User registration and login
- JWT token generation and validation
- Password hashing with bcrypt
- Rate limiting for security

## API Endpoints
- POST /auth/register - User registration
- POST /auth/login - User authentication
- GET /auth/me - Get current user info

## Recent Changes
- Initial authentication system implementation
                    """
                },
                "code_change": {
                    "commit_sha": "m3n4o5p6",
                    "commit_message": "docs: add deployment guide and update API documentation",
                    "diff_content": """
+## Deployment
+
+### Prerequisites
+- Python 3.9+
+- PostgreSQL 12+
+- Redis 6+ (for session management)
+
+### Environment Variables
+```env
+SECRET_KEY=your-secret-key
+DATABASE_URL=postgresql://user:pass@localhost/db
+REDIS_URL=redis://localhost:6379/0
+```
+
+### Docker Deployment
+```bash
+docker-compose up -d
+```

+## New API Endpoints (v2)
+- POST /auth/refresh - Refresh access token
+- POST /auth/logout - User logout
+- PATCH /auth/password - Change password
                    """
                }
            })
            
        return base_state

    async def test_change_analyzer_llm(self):
        """Change Analyzer LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Testing Change Analyzer (LLM Mode)")
        
        scenarios = ["feature_auth", "bugfix_database", "refactor_api"]
        results = {}
        
        for scenario in scenarios:
            print(f"  ğŸ“‹ Testing scenario: {scenario}")
            
            # State ìƒì„±
            state = self.create_realistic_state(scenario)
            
            # Change Analyzer ì‹¤í–‰ (LLM ëª¨ë“œ)
            try:
                result_state = change_analyzer_node(state, llm=self.llm, use_mock=False)
                
                # ê²°ê³¼ ê²€ì¦
                analysis_result = result_state.get("analysis_result", "")
                
                results[scenario] = {
                    "success": True,
                    "input": {
                        "changed_files": state.get("changed_files", []),
                        "commit_message": state.get("code_change", {}).get("commit_message", ""),
                        "diff_lines": len(state.get("code_change", {}).get("diff_content", "").split('\n'))
                    },
                    "output": {
                        "status": result_state.get("status"),
                        "analysis_length": len(analysis_result),
                        "analysis_content": analysis_result,
                        "has_korean": "í•œêµ­ì–´" in analysis_result or any(ord(c) > 127 for c in analysis_result),
                        "contains_technical_terms": any(term in analysis_result.lower() for term in ["jwt", "database", "api", "authentication", "connection"])
                    }
                }
                
                print(f"    âœ… Analysis completed: {len(analysis_result)} chars")
                
            except Exception as e:
                results[scenario] = {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__
                }
                print(f"    âŒ Analysis failed: {e}")
        
        # ê²°ê³¼ ì €ì¥
        self.save_test_result("change_analyzer", results)

    async def test_document_generator_llm(self):
        """Document Generator LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Testing Document Generator (LLM Mode)")
        
        scenarios = ["feature_auth", "bugfix_database", "refactor_api"]
        results = {}
        
        for scenario in scenarios:
            print(f"  ğŸ“‹ Testing scenario: {scenario}")
            
            # State ìƒì„± ë° ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            state = self.create_realistic_state(scenario)
            
            # ë¨¼ì € ë¶„ì„ ìˆ˜í–‰
            analyzed_state = change_analyzer_node(state, llm=self.llm, use_mock=False)
            
            try:
                # Document Generator ì‹¤í–‰ (LLM ëª¨ë“œ)
                final_state = document_generator_node(analyzed_state, llm=self.llm, use_mock=False)
                
                document_content = final_state.get("document_content", "")
                document_summary = final_state.get("document_summary", "")
                
                results[scenario] = {
                    "success": True,
                    "input": {
                        "analysis_result": analyzed_state.get("analysis_result", ""),
                        "changed_files": state.get("changed_files", []),
                        "scenario_type": scenario
                    },
                    "output": {
                        "status": final_state.get("status"),
                        "document_length": len(document_content),
                        "summary_length": len(document_summary),
                        "document_content": document_content,
                        "document_summary": document_summary,
                        "has_markdown": "##" in document_content or "#" in document_content,
                        "has_code_blocks": "```" in document_content,
                        "structure_analysis": {
                            "has_headers": document_content.count("#") > 0,
                            "has_lists": document_content.count("-") > 3,
                            "paragraph_count": document_content.count("\n\n")
                        }
                    }
                }
                
                print(f"    âœ… Document generated: {len(document_content)} chars, Summary: {len(document_summary)} chars")
                
            except Exception as e:
                results[scenario] = {
                    "success": False,
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "analysis_result": analyzed_state.get("analysis_result", "")
                }
                print(f"    âŒ Document generation failed: {e}")
        
        # ê²°ê³¼ ì €ì¥
        self.save_test_result("document_generator", results)

    async def test_document_update_llm(self):
        """ë¬¸ì„œ ì—…ë°ì´íŠ¸ LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Testing Document Update (LLM Mode)")
        
        # ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
        state = self.create_realistic_state("document_update")
        
        try:
            print("  ğŸ“‹ Step 1: Change Analysis")
            # 1. ë³€ê²½ì‚¬í•­ ë¶„ì„
            analyzed_state = change_analyzer_node(state, llm=self.llm, use_mock=False)
            
            print("  ğŸ“‹ Step 2: Document Update")
            # 2. ë¬¸ì„œ ì—…ë°ì´íŠ¸
            final_state = document_generator_node(analyzed_state, llm=self.llm, use_mock=False)
            
            # ê²°ê³¼ ë¶„ì„
            original_content = state.get("existing_document", {}).get("content", "")
            updated_content = final_state.get("document_content", "")
            
            result = {
                "success": True,
                "workflow": "document_update_llm",
                "steps": {
                    "1_analysis": {
                        "status": analyzed_state.get("status"),
                        "analysis_length": len(analyzed_state.get("analysis_result", "")),
                        "analysis_content": analyzed_state.get("analysis_result", "")
                    },
                    "2_document_update": {
                        "status": final_state.get("status"),
                        "original_length": len(original_content),
                        "updated_length": len(updated_content),
                        "content_changed": original_content != updated_content,
                        "summary": final_state.get("document_summary", ""),
                        "updated_content": updated_content
                    }
                },
                "quality_analysis": {
                    "content_coherence": "Recent Changes" in updated_content,
                    "preserves_structure": "## Overview" in updated_content,
                    "adds_new_information": len(updated_content) > len(original_content),
                    "maintains_formatting": updated_content.count("#") >= original_content.count("#"),
                    "integration_quality": {
                        "smooth_integration": "deployment" in updated_content.lower() or "í™˜ê²½ë³€ìˆ˜" in updated_content,
                        "proper_sectioning": updated_content.count("##") > original_content.count("##"),
                        "maintains_context": "authentication" in updated_content.lower()
                    }
                }
            }
            
            print(f"    âœ… Document updated: {len(original_content)} â†’ {len(updated_content)} chars")
            
        except Exception as e:
            result = {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }
            print(f"    âŒ Document update failed: {e}")
        
        # ê²°ê³¼ ì €ì¥
        self.save_test_result("document_update", result)

    async def test_quality_comparison(self):
        """Mock vs LLM í’ˆì§ˆ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª Testing Quality Comparison (Mock vs LLM)")
        
        scenario = "feature_auth"
        state = self.create_realistic_state(scenario)
        
        results = {}
        
        try:
            # Mock ëª¨ë“œ í…ŒìŠ¤íŠ¸
            print("  ğŸ“‹ Testing Mock Mode")
            mock_analyzed = change_analyzer_node(state, llm=None, use_mock=True)
            mock_document = document_generator_node(mock_analyzed, llm=None, use_mock=True)
            
            # LLM ëª¨ë“œ í…ŒìŠ¤íŠ¸
            print("  ğŸ“‹ Testing LLM Mode")
            llm_analyzed = change_analyzer_node(state, llm=self.llm, use_mock=False)
            llm_document = document_generator_node(llm_analyzed, llm=self.llm, use_mock=False)
            
            # í’ˆì§ˆ ë¹„êµ
            results = {
                "scenario": scenario,
                "mock_results": {
                    "analysis_length": len(mock_analyzed.get("analysis_result", "")),
                    "document_length": len(mock_document.get("document_content", "")),
                    "analysis_content": mock_analyzed.get("analysis_result", ""),
                    "document_content": mock_document.get("document_content", "")
                },
                "llm_results": {
                    "analysis_length": len(llm_analyzed.get("analysis_result", "")),
                    "document_length": len(llm_document.get("document_content", "")),
                    "analysis_content": llm_analyzed.get("analysis_result", ""),
                    "document_content": llm_document.get("document_content", "")
                },
                "quality_comparison": {
                    "analysis_detail_ratio": len(llm_analyzed.get("analysis_result", "")) / max(len(mock_analyzed.get("analysis_result", "")), 1),
                    "document_detail_ratio": len(llm_document.get("document_content", "")) / max(len(mock_document.get("document_content", "")), 1),
                    "llm_has_technical_depth": "JWT" in llm_analyzed.get("analysis_result", "") and "authentication" in llm_analyzed.get("analysis_result", ""),
                    "llm_document_structure": llm_document.get("document_content", "").count("##") > mock_document.get("document_content", "").count("##"),
                    "summary_quality": {
                        "mock_summary": mock_document.get("document_summary", ""),
                        "llm_summary": llm_document.get("document_summary", ""),
                        "llm_more_descriptive": len(llm_document.get("document_summary", "")) > len(mock_document.get("document_summary", ""))
                    }
                }
            }
            
            print(f"    âœ… Comparison completed")
            print(f"        Mock Analysis: {len(mock_analyzed.get('analysis_result', ''))} chars")
            print(f"        LLM Analysis: {len(llm_analyzed.get('analysis_result', ''))} chars") 
            print(f"        Mock Document: {len(mock_document.get('document_content', ''))} chars")
            print(f"        LLM Document: {len(llm_document.get('document_content', ''))} chars")
            
        except Exception as e:
            results = {
                "success": False,
                "error": str(e),
                "error_type": type(e).__name__
            }
            print(f"    âŒ Quality comparison failed: {e}")
        
        # ê²°ê³¼ ì €ì¥
        self.save_test_result("quality_comparison", results)

async def run_llm_tests():
    """ëª¨ë“  LLM í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Starting LLM Mode Integration Tests")
    print("=" * 70)
    
    try:
        tester = TestLLMMode()
        
        # ê° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await tester.test_change_analyzer_llm()
        await tester.test_document_generator_llm()
        await tester.test_document_update_llm()
        await tester.test_quality_comparison()
        
        print("\n" + "=" * 70)
        print("ğŸ‰ All LLM tests completed! Check test_results/ directory for detailed reports.")
        print(f"ğŸ“ Results saved with session ID: {tester.test_session}")
        
    except ValueError as e:
        print(f"âŒ Configuration Error: {e}")
        print("ğŸ’¡ Please set OPENAI_API_KEY environment variable")
    except Exception as e:
        print(f"âŒ Test execution failed: {e}")

if __name__ == "__main__":
    asyncio.run(run_llm_tests())