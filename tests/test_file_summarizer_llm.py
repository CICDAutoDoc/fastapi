"""
File Summarizer Node LLM í…ŒìŠ¤íŠ¸
LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ íŒŒì¼ ìš”ì•½ ê²°ê³¼ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
"""

import sys
import os
import json
import asyncio
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__)))
sys.path.insert(0, project_root)

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

from domain.langgraph.nodes.file_summarizer_node import file_summarizer_node
from domain.langgraph.nodes.file_parser_node import file_parser_node
from domain.langgraph.document_state import DocumentState


class FileSummarizerTester:
    """File Summarizer LLM í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self, openai_api_key: str | None = None):
        """
        í…ŒìŠ¤í„° ì´ˆê¸°í™”
        
        Args:
            openai_api_key: OpenAI API í‚¤. Noneì´ë©´ í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
        """
        self.api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            print("âš ï¸  OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Mock ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        
        self.test_results = []
        self.project_root = Path(project_root)
    
    def get_test_files(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸í•  íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        test_files = [
            "main.py",
            "models.py", 
            "domain/langgraph/nodes/file_summarizer_node.py",
            "domain/langgraph/nodes/file_parser_node.py",
            "domain/langgraph/document_service.py",
            "app/endpoints/chat.py",
            "domain/user/service.py"
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
        existing_files = []
        for file_path in test_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                existing_files.append(file_path)
            else:
                print(f"âš ï¸  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}")
        
        return existing_files
    
    def create_test_state(self, file_paths: List[str]) -> DocumentState:
        """í…ŒìŠ¤íŠ¸ìš© DocumentState ìƒì„±"""
        state = DocumentState()
        state["repository_path"] = str(self.project_root)
        # target_filesëŠ” DocumentStateì— ì—†ìœ¼ë¯€ë¡œ ì œê±°
        state["status"] = "parsing_files"
        
        return state
    
    def run_file_parser(self, state: DocumentState) -> DocumentState:
        """íŒŒì¼ íŒŒì„œ ì‹¤í–‰"""
        print("\nğŸ“ íŒŒì¼ íŒŒì‹± ì¤‘...")
        
        # íŒŒì¼ íŒŒì„œ ë…¸ë“œ ì‹¤í–‰ - í…ŒìŠ¤íŠ¸ íŒŒì¼ë“¤ì„ ì§ì ‘ ì„¤ì •
        # target_files ëŒ€ì‹  ì§ì ‘ parsed_filesë¥¼ ìƒì„±
        test_files = [
            "main.py",
            "models.py", 
            "domain/langgraph/nodes/file_summarizer_node.py",
            "domain/langgraph/nodes/file_parser_node.py",
            "domain/langgraph/document_service.py"
        ]
        
        # ì˜ˆì‹œ ë°ì´í„°ë¡œ parsed_files ì„¤ì •
        parsed_files = []
        for file_path in test_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                try:
                    content = full_path.read_text(encoding='utf-8', errors='ignore')
                    parsed_files.append({
                        "file_path": file_path,
                        "language": "python",
                        "full_code": content,
                        "functions": [],
                        "classes": [],
                        "imports": [],
                        "loc": len(content.splitlines()),
                        "complexity_score": 1
                    })
                except Exception as e:
                    print(f"âš ï¸  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
        
        state["parsed_files"] = parsed_files
        print(f"âœ… {len(parsed_files)}ê°œ íŒŒì¼ íŒŒì‹± ì™„ë£Œ")
        
        return state
    
    def run_file_summarizer(self, state: DocumentState, use_llm: bool = True) -> DocumentState:
        """íŒŒì¼ ìš”ì•½ê¸° ì‹¤í–‰"""
        mode = "LLM" if use_llm and self.api_key else "Mock"
        print(f"\nğŸ“ íŒŒì¼ ìš”ì•½ ì¤‘ ({mode} ëª¨ë“œ)...")
        
        # íŒŒì¼ ìš”ì•½ê¸° ë…¸ë“œ ì‹¤í–‰
        updated_state = file_summarizer_node(
            state,
            use_mock=not (use_llm and self.api_key),
            openai_api_key=self.api_key,
            include_full_code=True  # ì „ì²´ ì½”ë“œ í¬í•¨í•˜ì—¬ ë” ìƒì„¸í•œ ìš”ì•½ ìƒì„±
        )
        
        if "error" in updated_state:
            print(f"âŒ íŒŒì¼ ìš”ì•½ ì˜¤ë¥˜: {updated_state.get('error')}")
            return updated_state
        
        file_summaries = updated_state.get("file_summaries", [])
        print(f"âœ… {len(file_summaries or [])}ê°œ íŒŒì¼ ìš”ì•½ ì™„ë£Œ")
        
        return updated_state
    
    def analyze_summary_quality(self, summary: Dict[str, Any]) -> Dict[str, Any]:
        """ìš”ì•½ í’ˆì§ˆ ë¶„ì„"""
        quality_score = 0
        issues = []
        good_points = []
        
        summary_data = summary.get("summary", {})
        
        # 1. í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        required_fields = ["purpose", "role", "key_features"]
        missing_fields = [field for field in required_fields if not summary_data.get(field)]
        
        if not missing_fields:
            quality_score += 20
            good_points.append("ëª¨ë“  í•„ìˆ˜ í•„ë“œê°€ ì¡´ì¬í•¨")
        else:
            issues.append(f"ëˆ„ë½ëœ í•„ë“œ: {missing_fields}")
        
        # 2. ë‚´ìš©ì˜ êµ¬ì²´ì„± ì²´í¬
        purpose = summary_data.get("purpose", "")
        if purpose and len(purpose) > 20:
            quality_score += 20
            good_points.append("ëª©ì  ì„¤ëª…ì´ êµ¬ì²´ì ì„")
        else:
            issues.append("ëª©ì  ì„¤ëª…ì´ ë„ˆë¬´ ê°„ë‹¨í•¨")
        
        # 3. ì£¼ìš” ê¸°ëŠ¥ ë¶„ì„
        key_features = summary_data.get("key_features", [])
        if len(key_features) >= 3:
            quality_score += 20
            good_points.append(f"{len(key_features)}ê°œì˜ ì£¼ìš” ê¸°ëŠ¥ ì‹ë³„ë¨")
        else:
            issues.append("ì£¼ìš” ê¸°ëŠ¥ì´ ì¶©ë¶„íˆ ì‹ë³„ë˜ì§€ ì•ŠìŒ")
        
        # 4. ë³µì¡ë„ í‰ê°€ ì¡´ì¬
        complexity = summary_data.get("complexity_assessment", "")
        if complexity and complexity != "unknown":
            quality_score += 15
            good_points.append("ë³µì¡ë„ í‰ê°€ê°€ ìˆ˜í–‰ë¨")
        else:
            issues.append("ë³µì¡ë„ í‰ê°€ê°€ ëˆ„ë½ë¨")
        
        # 5. ì˜ì¡´ì„± ë¶„ì„
        dependencies = summary_data.get("dependency_analysis", [])
        if dependencies:
            quality_score += 15
            good_points.append("ì˜ì¡´ì„± ë¶„ì„ì´ ìˆ˜í–‰ë¨")
        else:
            issues.append("ì˜ì¡´ì„± ë¶„ì„ì´ ëˆ„ë½ë¨")
        
        # 6. ìœ ì§€ë³´ìˆ˜ì„± í‰ê°€
        maintainability = summary_data.get("maintainability", "")
        if maintainability and maintainability != "unknown":
            quality_score += 10
            good_points.append("ìœ ì§€ë³´ìˆ˜ì„± í‰ê°€ê°€ ìˆ˜í–‰ë¨")
        else:
            issues.append("ìœ ì§€ë³´ìˆ˜ì„± í‰ê°€ê°€ ëˆ„ë½ë¨")
        
        return {
            "quality_score": quality_score,
            "grade": self._get_quality_grade(quality_score),
            "good_points": good_points,
            "issues": issues
        }
    
    def _get_quality_grade(self, score: int) -> str:
        """í’ˆì§ˆ ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 90:
            return "A+"
        elif score >= 80:
            return "A"
        elif score >= 70:
            return "B+"
        elif score >= 60:
            return "B"
        elif score >= 50:
            return "C"
        else:
            return "D"
    
    def print_summary_details(self, summary: Dict[str, Any], quality_analysis: Dict[str, Any]):
        """ìš”ì•½ ìƒì„¸ ì •ë³´ ì¶œë ¥"""
        print(f"\nğŸ“„ íŒŒì¼: {summary.get('file_path', 'Unknown')}")
        print(f"ğŸ”¤ ì–¸ì–´: {summary.get('language', 'Unknown')}")
        print(f"ğŸ¯ ìƒì„± ë°©ì‹: {summary.get('generation_method', 'Unknown')}")
        
        summary_data = summary.get("summary", {})
        
        print(f"\nğŸ“ ìš”ì•½ ë‚´ìš©:")
        print(f"  â€¢ ëª©ì : {summary_data.get('purpose', 'Unknown')}")
        print(f"  â€¢ ì—­í• : {summary_data.get('role', 'Unknown')}")
        print(f"  â€¢ ë³µì¡ë„: {summary_data.get('complexity_assessment', 'Unknown')}")
        print(f"  â€¢ ìœ ì§€ë³´ìˆ˜ì„±: {summary_data.get('maintainability', 'Unknown')}")
        
        key_features = summary_data.get('key_features', [])
        if key_features:
            print(f"  â€¢ ì£¼ìš” ê¸°ëŠ¥:")
            for feature in key_features[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(f"    - {feature}")
        
        print(f"\nğŸ“Š í†µê³„:")
        print(f"  â€¢ í•¨ìˆ˜: {summary_data.get('functions_count', 0)}ê°œ")
        print(f"  â€¢ í´ë˜ìŠ¤: {summary_data.get('classes_count', 0)}ê°œ")
        print(f"  â€¢ ì„í¬íŠ¸: {summary_data.get('imports_count', 0)}ê°œ")
        print(f"  â€¢ LOC: {summary_data.get('loc', 0)}ì¤„")
        
        print(f"\nğŸ† í’ˆì§ˆ í‰ê°€:")
        print(f"  â€¢ ì ìˆ˜: {quality_analysis['quality_score']}/100")
        print(f"  â€¢ ë“±ê¸‰: {quality_analysis['grade']}")
        
        if quality_analysis['good_points']:
            print(f"  â€¢ ì¥ì :")
            for point in quality_analysis['good_points']:
                print(f"    âœ… {point}")
        
        if quality_analysis['issues']:
            print(f"  â€¢ ê°œì„ ì :")
            for issue in quality_analysis['issues']:
                print(f"    âš ï¸  {issue}")
    
    def save_test_results(self, results: List[Dict[str, Any]], filename: str | None = None):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"file_summarizer_test_results_{timestamp}.json"
        
        results_dir = self.project_root / "test_results"
        results_dir.mkdir(exist_ok=True)
        
        output_path = results_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {output_path}")
        
        # ë§ˆí¬ë‹¤ìš´ ë²„ì „ë„ ì €ì¥
        md_filename = filename.replace('.json', '.md')
        md_path = results_dir / md_filename
        self.save_markdown_report(results, md_path)
        print(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥: {md_path}")
    
    def save_markdown_report(self, results: List[Dict[str, Any]], output_path: Path):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# File Summarizer LLM í…ŒìŠ¤íŠ¸ ê²°ê³¼\n\n")
            f.write(f"**í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # ì „ì²´ í†µê³„
            total_files = len(results)
            avg_quality = sum(r['quality_analysis']['quality_score'] for r in results) / total_files if total_files > 0 else 0
            
            f.write("## ğŸ“Š ì „ì²´ í†µê³„\n\n")
            f.write(f"- **í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜**: {total_files}ê°œ\n")
            f.write(f"- **í‰ê·  í’ˆì§ˆ ì ìˆ˜**: {avg_quality:.1f}/100\n")
            f.write(f"- **í‰ê·  ë“±ê¸‰**: {self._get_quality_grade(int(avg_quality))}\n\n")
            
            # íŒŒì¼ë³„ ìƒì„¸ ê²°ê³¼
            f.write("## ğŸ“ íŒŒì¼ë³„ ìƒì„¸ ê²°ê³¼\n\n")
            
            for i, result in enumerate(results, 1):
                summary = result['summary']
                quality = result['quality_analysis']
                summary_data = summary.get('summary', {})
                
                f.write(f"### {i}. {summary.get('file_path', 'Unknown')}\n\n")
                f.write(f"- **ì–¸ì–´**: {summary.get('language', 'Unknown')}\n")
                f.write(f"- **ìƒì„± ë°©ì‹**: {summary.get('generation_method', 'Unknown')}\n")
                f.write(f"- **í’ˆì§ˆ ì ìˆ˜**: {quality['quality_score']}/100 ({quality['grade']})\n\n")
                
                f.write("#### ğŸ“ ìš”ì•½ ë‚´ìš©\n\n")
                f.write(f"**ëª©ì **: {summary_data.get('purpose', 'Unknown')}\n\n")
                f.write(f"**ì—­í• **: {summary_data.get('role', 'Unknown')}\n\n")
                
                key_features = summary_data.get('key_features', [])
                if key_features:
                    f.write("**ì£¼ìš” ê¸°ëŠ¥**:\n")
                    for feature in key_features:
                        f.write(f"- {feature}\n")
                    f.write("\n")
                
                f.write("#### ğŸ“Š í†µê³„ ì •ë³´\n\n")
                f.write(f"- í•¨ìˆ˜: {summary_data.get('functions_count', 0)}ê°œ\n")
                f.write(f"- í´ë˜ìŠ¤: {summary_data.get('classes_count', 0)}ê°œ\n")
                f.write(f"- ì„í¬íŠ¸: {summary_data.get('imports_count', 0)}ê°œ\n")
                f.write(f"- LOC: {summary_data.get('loc', 0)}ì¤„\n\n")
                
                if quality['good_points']:
                    f.write("#### âœ… ì¥ì \n\n")
                    for point in quality['good_points']:
                        f.write(f"- {point}\n")
                    f.write("\n")
                
                if quality['issues']:
                    f.write("#### âš ï¸ ê°œì„ ì \n\n")
                    for issue in quality['issues']:
                        f.write(f"- {issue}\n")
                    f.write("\n")
                
                f.write("---\n\n")
    
    async def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "="*80)
        print("ğŸ§ª File Summarizer LLM ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*80)
        
        # 1. í…ŒìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        test_files = self.get_test_files()
        if not test_files:
            print("âŒ í…ŒìŠ¤íŠ¸í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"ğŸ“ í…ŒìŠ¤íŠ¸ ëŒ€ìƒ íŒŒì¼: {len(test_files)}ê°œ")
        for file_path in test_files:
            print(f"  - {file_path}")
        
        # 2. ìƒíƒœ ìƒì„± ë° íŒŒì¼ íŒŒì‹±
        state = self.create_test_state(test_files)
        state = self.run_file_parser(state)
        
        error_msg = state.get("error")
        if error_msg:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨: {error_msg}")
            return
        
        # 3. íŒŒì¼ ìš”ì•½ ì‹¤í–‰ (LLM ëª¨ë“œ)
        state = self.run_file_summarizer(state, use_llm=True)
        
        error_msg = state.get("error")
        if error_msg:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨: {error_msg}")
            return
        
        # 4. ê²°ê³¼ ë¶„ì„
        file_summaries = state.get("file_summaries", [])
        if not file_summaries:
            print("âŒ ìš”ì•½ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        test_results = []
        
        print(f"\nğŸ“Š ìš”ì•½ ê²°ê³¼ ë¶„ì„ ì¤‘...")
        for summary in file_summaries:
            quality_analysis = self.analyze_summary_quality(summary)
            
            result = {
                "summary": summary,
                "quality_analysis": quality_analysis,
                "timestamp": datetime.now().isoformat()
            }
            test_results.append(result)
            
            # ê°œë³„ ê²°ê³¼ ì¶œë ¥
            self.print_summary_details(summary, quality_analysis)
            print("\n" + "-"*60)
        
        # 5. ì „ì²´ ê²°ê³¼ ìš”ì•½
        if test_results:
            avg_quality = sum(r['quality_analysis']['quality_score'] for r in test_results) / len(test_results)
            print(f"\nğŸ† ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
            print(f"  â€¢ í…ŒìŠ¤íŠ¸ íŒŒì¼: {len(test_results)}ê°œ")
            print(f"  â€¢ í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.1f}/100")
            print(f"  â€¢ í‰ê·  ë“±ê¸‰: {self._get_quality_grade(int(avg_quality))}")
            
            # í’ˆì§ˆë³„ ë¶„í¬
            grades = [r['quality_analysis']['grade'] for r in test_results]
            from collections import Counter
            grade_counts = Counter(grades)
            print(f"  â€¢ ë“±ê¸‰ ë¶„í¬:")
            for grade in ['A+', 'A', 'B+', 'B', 'C', 'D']:
                if grade in grade_counts:
                    print(f"    - {grade}: {grade_counts[grade]}ê°œ")
        
        # 6. ê²°ê³¼ ì €ì¥
        self.save_test_results(test_results)
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return test_results


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ File Summarizer LLM í…ŒìŠ¤íŠ¸ ë„êµ¬")
    print("="*50)
    
    # OpenAI API í‚¤ í™•ì¸
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\nâš ï¸  í™˜ê²½ë³€ìˆ˜ì—ì„œ OPENAI_API_KEYë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("Mock ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        api_key = None
    
    # í…ŒìŠ¤í„° ìƒì„± ë° ì‹¤í–‰
    tester = FileSummarizerTester(api_key)
    await tester.run_comprehensive_test()


if __name__ == "__main__":
    asyncio.run(main())